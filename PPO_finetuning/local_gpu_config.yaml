lamorel_args:
  log_level: info
  allow_subgraph_use_whith_gradient: true
  distributed_setup_args:
    n_rl_processes: 1
    n_llm_processes: 1
  accelerate_args:
    config_file: /homedtcl/mmahaut/projects/lamorel/zoo/llm2llm/configs/accelerate/default_config.yaml
    machine_rank: 0
    num_machines: 1
  llm_args:
    model_type: causal
    model_path: gpt2
    pretrained: true
    minibatch_size: 1
    parallelism:
      use_gpu: true
      model_parallelism_size: 2
      synchronize_gpus_after_scoring: false
      empty_cuda_cache_after_scoring: false
  updater_args:
rl_script_args:
  path: /homedtcl/mmahaut/projects/lamorel/zoo/llm2llm/train.py
  dataset_path: /homedtcl/mmahaut/projects/llm2llm/data/mindless_dataset_randomized_train.txt
  epochs: 20
  steps_per_epoch: 2
  num_candidates: 3
  batch_size: 50
  max_new_tokens: 30
  top_k: 10
  ppo_epochs: 10
  lam: 0.99
  gamma: 0.99
  clip_ration: 0.2
  target_kl: 0.01
  max_ep_len: 3
  lr: 1e-6
  entropy_coef: 0.01
  value_loss_coef: 0
  ce_coef: 0.1
  clip_eps: 0.1
  save_freq: 10
  affixes:
  log_dir: /homedtcl/mmahaut/projects/llm2llm/fine_tune
  log_file: /homedtcl/mmahaut/projects/llm2llm/fine_tune/diaLogOPT.txt
