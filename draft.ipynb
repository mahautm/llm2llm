{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import spacy\n",
    "from spacy import load\n",
    "import numerizer\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from transformers import Trainer\n",
    "\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e47e5cf182e432eb9c0d40ad17c3a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_inputs(input_1, input_2):\n",
    "    \"\"\"takes two tokenized inputs and combines them into one tokenized input\"\"\"\n",
    "    output = {}\n",
    "    for key in input_1:\n",
    "        assert key in input_2, \"The two inputs should have the same keys\"\n",
    "        output[key] = torch.cat([input_1[key], input_2[key]], dim=1)\n",
    "    return output\n",
    "    \n",
    "def format_as_input(sequence):\n",
    "    \"\"\"takes a sequence of tokens and formats it as an input for the model\"\"\"\n",
    "    return {\n",
    "        \"input_ids\": sequence,\n",
    "        \"attention_mask\": torch.ones_like(sequence),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, input, max_length):\n",
    "    \"\"\"generates logits by selecting the highest probability token at each step using model.forward()\"\"\"\n",
    "    if isinstance(input, torch.Tensor):\n",
    "        input = format_as_input(input)\n",
    "    all_logits = []\n",
    "    generated_tokens = []\n",
    "    logits = model.forward(**input).logits\n",
    "    for _ in range(max_length):\n",
    "        next_token_logits = logits[:, -1, :]\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(-1)\n",
    "        input = combine_inputs(input, format_as_input(next_token))\n",
    "        logits = model.forward(**input).logits\n",
    "        all_logits.append(next_token_logits)\n",
    "        generated_tokens.append(next_token)\n",
    "        # check if the model has generated an end token\n",
    "        if next_token.all() == model.config.eos_token_id:\n",
    "            break\n",
    "    return all_logits, torch.cat(generated_tokens, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4564, grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "kl_regularisation = torch.nn.functional.kl_div(lm1b_logsoftmax, old_logits, reduction=\"batchmean\", log_target=True)\n",
    "print(kl_regularisation*10e-8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"\\n\\n\\n\" \"\\n\\n\" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \"', ' all all all all all all all all all all all all all all all all all all all all all all all all all all', ' have have have all all have have all have all have all have all have all have all have all have all have all have all', '? all all all all all all all all all all all all all all all all all all all all all all all all all', ') and and and and and and and and and and and and and and and and and and and and and and and and and', ' of a, and all of a, and all of a, and all of a, and all of a and all of a', ' only thing that matters is that you have six hazelnuts.\\n\\n\\nYou have six hazelnuts.\\nYou have', '\\n\\n\\n.\\n\\n.\\n\\n.\\n\\n.\\n.\\n\\n.\\n\\n.\\n\\n.\\n\\n', ' then all all all all all all all all all all all all all all all all all all all all all all all all all', ', and how all of all of the above is a very good thing.\\n\\nI have been to the same places and have', ' have have have have have\"\\n\\n\"I have had a lot of trouble with the word \\'I\\' in the English language', '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nThe following is a list of', ' all all all all all all all all all all all all all all all all all all all all all all all all all all', ' all all all all all all all all all all all all all all all all all all all all all all all all all all', ' all all all all all all all all all all all all all all all all all all all all all all all all all all', ' all all all of all all all all all all all all all all all all all all all all all all all all all all', '\" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \" \"', ' all all all be all all all all all all all all all all all all all all all all all all all all all all', '. have, or\"\"have\" or\"have\"\" or\"have\" or\"have\" or\"have\" or', '\\n\" and and what\"\\n\\n\" and \" and \" and \" and \" and \" and \" and \" and \" and', ' it of all all all all all of all all all all all all all all all all all all all all all all all all', '\" as as as will be.\\n\\n\\nAnd the same is true of the same of the same of the same of the', ' all all then all all all all all all all all all all all all all all all all all all all all all all all', ' had, sex have had have had had had had had had had had had had had had had had had had had had had had', ' all all the and all and all and all and all and all and all and all and all and all and all and all and']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\u203445\\AppData\\Local\\Temp\\ipykernel_14932\\1630933896.py:46: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  lm1b_softmax = torch.nn.functional.softmax(lm1b_logits)\n",
      "C:\\Users\\u203445\\AppData\\Local\\Temp\\ipykernel_14932\\1630933896.py:47: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  lm1b_logsoftmax = torch.nn.functional.log_softmax(lm1b_logits)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' \" \" \"', ' all all all', ' have all have', ' all all all', ' and and and', ', and all', ' six hazel', '.\\n\\n', ' all all all', ' been to the', '. I have', ' the most common', ' all all all', ' all all all', ' all all all', ' all all all', ' \" \" \"', ' all all all', '\" or\"', ' \" and \"', ' all all all', ' same of the', ' all all all', ' had had had', ' all and all']\n",
      "cross ent loss:  9.227156639099121 policy loss:  -4.321336447314292e-14 reward:  4.38846838236211e-15 acc  0.0 kl  0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' all all all all all all all all all all all all all all all all all all all all all all all all all', '?????????????????????????', ' all of all of all of all of all of all of all of all of all of all of all of all of all', ' all all all all the all all all all all all all all all all all all all all all all all all all all', 'how\\n\\n\\n\\nand\\n\\nand\\nand\\n\\nand\\n\\n\\nand\\n\\nand\\n\\nand\\n', ',,,?,,,,,,,,,,,,,?,,,,,,,', '\\n\\n\\n\\nwh\\n\\n\\nwh\\n\\nwh\\nwh\\n\\nwh\\n\\nwh\\n\\nwh\\n\\n', ' of people who are able to see the number of the people who are able to see the number of the number of the number', ' the the all the\\n\\nand the the the the the the the the the the the the the the the the the the', ' other other not all all the other\\n\\nand the other\\n\\nand the other\\n\\nand the other\\n\\nand', '. all the the the????????????????????', ' sure what the answer is.\\n\\nI am not sure what the answer is.\\n\\nI am not sure what the', ' that the network is not a network.\\n\\nThe network is not a network.\\n\\nThe network is not a network', ',,,,,,,,,,,,,,,,,,,,,,,,,', ' of of of many of of of of of of of of of of of of of of of of of of of of of', ' all of of any of any of any of any of any of any of any of any of any of any of any of', ',,,,, and, and, and, and, and, and, and, and, and, and,', ' all,, all, all, all, all, all, all, all, all, all, all, all,', ' will all all all all all all all all all all all all all all all all all all all all all all all all', ',,, is, is, is, is, is, is, is, is, is, is, is,', ' many all all of all all all all all all all all all all all all all all all all all all all all all', ' to say that is to say that you have been.\\n\\n\\nThe only way to say that you have been is to', ' all all all all all all all all all all all all all all all all all all all all all all all all all', '.........................', ' no and the no.\"\\n\\nThe only thing that is not a thing is the word \"totem.\"\\n\\n']\n",
      "[' all all all', '???', ' of all of', ' all all all', 'and\\n\\n', ',,,', 'wh\\n\\n', ' of the people', ' the the the', ' the other\\n', '???', ' answer is.', '. The network', ',,,', ' of of of', ' any of any', ' and, and', ' all, all', ' all all all', ' is, is', ' all all all', ' say that you', ' all all all', '...', 'The only thing']\n",
      "cross ent loss:  8.255084991455078 policy loss:  -4.142527033207388e-12 reward:  4.596304890511216e-13 acc  0.0 kl  0.09044938534498215\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [81], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m     35\u001b[0m \u001b[39m# forward pass question -> lm1 (+ context) -> lm2 -> lm1 -> answer\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[39m# at every step we remove the input from the output\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m lm1_logits, lm1_tokens \u001b[39m=\u001b[39m generate(model, questions, \u001b[39m10\u001b[39;49m)\n\u001b[0;32m     38\u001b[0m lm2_input \u001b[39m=\u001b[39m combine_inputs(\n\u001b[0;32m     39\u001b[0m     context,\n\u001b[0;32m     40\u001b[0m     format_as_input(lm1_tokens),\n\u001b[0;32m     41\u001b[0m     )\n\u001b[0;32m     42\u001b[0m lm2_outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mgenerate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mlm2_input, return_dict_in_generate\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, output_scores\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, max_new_tokens\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n",
      "Cell \u001b[1;32mIn [26], line 12\u001b[0m, in \u001b[0;36mgenerate\u001b[1;34m(model, input, max_length)\u001b[0m\n\u001b[0;32m     10\u001b[0m next_token \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39margmax(next_token_logits, dim\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m combine_inputs(\u001b[39minput\u001b[39m, format_as_input(next_token))\n\u001b[1;32m---> 12\u001b[0m logits \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mforward(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m)\u001b[39m.\u001b[39mlogits\n\u001b[0;32m     13\u001b[0m all_logits\u001b[39m.\u001b[39mappend(next_token_logits)\n\u001b[0;32m     14\u001b[0m generated_tokens\u001b[39m.\u001b[39mappend(next_token)\n",
      "File \u001b[1;32mc:\\Users\\u203445\\Miniconda3\\envs\\willy\\lib\\site-packages\\transformers\\models\\gpt2\\modeling_gpt2.py:1114\u001b[0m, in \u001b[0;36mGPT2LMHeadModel.forward\u001b[1;34m(self, input_ids, past_key_values, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1111\u001b[0m     torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mset_device(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mfirst_device)\n\u001b[0;32m   1112\u001b[0m     hidden_states \u001b[39m=\u001b[39m hidden_states\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head\u001b[39m.\u001b[39mweight\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m-> 1114\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlm_head(hidden_states)\n\u001b[0;32m   1116\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1117\u001b[0m \u001b[39mif\u001b[39;00m labels \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1118\u001b[0m     \u001b[39m# Shift so that tokens < n predict n\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\u203445\\Miniconda3\\envs\\willy\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\u203445\\Miniconda3\\envs\\willy\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from archs import AdapterModel\n",
    "from data import FruitDataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = \"cpu\"\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "# Add a new adapter\n",
    "model.add_adapter(\"fine_tune\")\n",
    "# Activate the adapter for training\n",
    "model.train_adapter(\"fine_tune\")\n",
    "# get the tokenizer, setup padding\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "# Load the dataset\n",
    "dataset = FruitDataset(\".\\data\\mindless_dataset_randomized_train.txt\")\n",
    "# Load the dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=25, shuffle=True)\n",
    "# setup the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "old_logits = None\n",
    "# Train the model\n",
    "for epoch in range(10):\n",
    "    for batch in dataloader:\n",
    "        # get the data, tokenize it, and put it on the GPU\n",
    "        questions = tokenizer.batch_encode_plus(list(batch[0]), return_tensors=\"pt\", padding=True).to(device)\n",
    "        context = tokenizer.batch_encode_plus(list(batch[1]), return_tensors=\"pt\", padding=True).to(device)\n",
    "        answers = tokenizer.batch_encode_plus(list(batch[2]), return_tensors=\"pt\", padding=\"max_length\",max_length=3).to(device)\n",
    "        # zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass question -> lm1 (+ context) -> lm2 -> lm1 -> answer\n",
    "        # at every step we remove the input from the output\n",
    "        lm1_logits, lm1_tokens = generate(model, questions, 10)\n",
    "        lm2_input = combine_inputs(\n",
    "            context,\n",
    "            format_as_input(lm1_tokens),\n",
    "            )\n",
    "        lm2_outputs = model.generate(**lm2_input, return_dict_in_generate=True, output_scores=True, max_new_tokens=20)\n",
    "        print(tokenizer.batch_decode(lm2_outputs.sequences[:, len(lm2_input[\"input_ids\"]):]))\n",
    "        lm1b_logits, lm1b_tokens = generate(model, lm2_outputs.sequences[:, len(lm2_input[\"input_ids\"]):], 3)\n",
    "        lm1b_logits = torch.cat(lm1b_logits)\n",
    "        lm1b_softmax = torch.nn.functional.softmax(lm1b_logits)\n",
    "        lm1b_logsoftmax = torch.nn.functional.log_softmax(lm1b_logits)\n",
    "        # compute the loss\n",
    "        # policy loss\n",
    "        # select the logits of the correct answer\n",
    "        expected_logits = torch.gather(lm1b_softmax, 1, answers.input_ids.flatten().unsqueeze(1))\n",
    "        # multiply for token from the same sequence, mean across batch\n",
    "        R = expected_logits.reshape(-1,len(answers.input_ids[0])).prod(dim=1).mean()\n",
    "        # compute according to policy choices for first message\n",
    "        policy_loss = ((lm1b_logsoftmax * R).mean(dim=1)*answers.attention_mask.flatten()).mean()\n",
    "        # ppo : substract kl divergence between old and new policy\n",
    "        if old_logits is not None:\n",
    "            kl_regularisation = torch.nn.functional.kl_div(lm1b_logsoftmax, old_logits, reduction=\"batchmean\", log_target=True)\n",
    "        else:\n",
    "            kl_regularisation = torch.Tensor([0])\n",
    "        old_logits = lm1b_logsoftmax.detach()\n",
    "        # total loss = crossent loss + policy loss\n",
    "        crossent_loss = (torch.nn.functional.cross_entropy(lm1b_logits, answers.input_ids.flatten(), reduction=\"none\")*answers.attention_mask.flatten()).mean()\n",
    "        loss = crossent_loss + policy_loss - kl_regularisation\n",
    "        print(tokenizer.batch_decode(lm1b_tokens))\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "        # print the loss\n",
    "        accuracy = (lm1b_tokens == answers.input_ids).float().mean()\n",
    "        print(\"cross ent loss: \", crossent_loss.item(), \"policy loss: \", policy_loss.item(), \"reward: \", R.item(),\"acc \", accuracy.item(), \"kl \", kl_regularisation.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('willy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6a04f62cc195c60c1b3feeb5e26e1fcc0c64fda7edb78d2917699dcaeca732b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
